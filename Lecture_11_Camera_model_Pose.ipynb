{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KMUTNB\\IEE\\Image Processing\\010723305-Computer-Vision/camera_params/monocular_camera_params/\n",
      "Camera matrix\n",
      "[[1.29968396e+03 0.00000000e+00 6.32814378e+02]\n",
      " [0.00000000e+00 1.29933998e+03 4.74721903e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Len distortion\n",
      "[[-0.40795692  0.24741507  0.00200567 -0.00202735 -0.09445672]]\n"
     ]
    }
   ],
   "source": [
    "params_dir = os.getcwd()+'/camera_params/monocular_camera_params/'\n",
    "print(params_dir)\n",
    "\n",
    "#load camera parameters\n",
    "K = np.load(params_dir+'K.npy')\n",
    "dist = np.load(params_dir+'dist.npy')\n",
    "\n",
    "print(\"Camera matrix\")\n",
    "print(K)\n",
    "print(\"Len distortion\")\n",
    "print(dist)\n",
    "\n",
    "chessboard_size = (8,6) # Your chessboard size\n",
    "# iterative termination criteria, maximum iterationm and epsilon\n",
    "term_criteria = (cv2.TermCriteria_EPS+ cv2.TermCriteria_MAX_ITER, 30, 0.001)\n",
    "# Defining the world coordinates for 3D points\n",
    "objp = np.zeros((1, chessboard_size[0] * chessboard_size[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1, 2)\n",
    "# 3D object for visualization\n",
    "axis = np.float32([[3,0,0], [0,3,0], [0,0,-3]]).reshape(-1,3)\n",
    "box = np.float32([[0,0,0], [0,4,0], [4,4,0], [4,0,0],\n",
    "                   [0,0,-4],[0,4,-4],[4,4,-4],[4,0,-4] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_axis(img, corners, imgpts) :\n",
    "    corner = tuple(corners[0].ravel().astype(int))\n",
    "    img = cv2.line(img, corner, tuple(imgpts[0].ravel().astype(int)), (255,0,0), 10)\n",
    "    img = cv2.line(img, corner, tuple(imgpts[1].ravel().astype(int)), (0,255,0), 10)\n",
    "    img = cv2.line(img, corner, tuple(imgpts[2].ravel().astype(int)), (0,0,255), 10)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(img, corners, imgpts) :\n",
    "    imgpts = np.int32(imgpts).reshape(-1,2)\n",
    "    # draw ground floor in green\n",
    "    img = cv2.drawContours(img, [imgpts[:4]],-1,(0,255,0),-3)\n",
    "    # draw pillars in blue color\n",
    "    for i,j in zip(range(4),range(4,8)):\n",
    "        img = cv2.line(img, tuple(imgpts[i]), tuple(imgpts[j]),(255),10)\n",
    "    # draw top layer in red color\n",
    "    img = cv2.drawContours(img, [imgpts[4:]],-1,(0,0,255),10)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_image(frame,im_src, pts_src, pts_dst):\n",
    "    \n",
    "    # Calculate Homography\n",
    "    h, status = cv2.findHomography(pts_src, pts_dst)\n",
    "\n",
    "    # Warp source image to destination based on homography\n",
    "    warped_image = cv2.warpPerspective(im_src, h, (frame.shape[1],frame.shape[0]))\n",
    "            \n",
    "    # Prepare a mask representing region to copy from the warped image into the original frame.\n",
    "    mask = np.zeros([frame.shape[0], frame.shape[1]], dtype=np.uint8)\n",
    "    cv2.fillConvexPoly(mask, np.int32(pts_dst), (255, 255, 255), cv2.LINE_AA)\n",
    "    im_out = cv2.add(frame, warped_image, mask=cv2.bitwise_not(mask))\n",
    "    im_out = cv2.add(im_out, warped_image)\n",
    "    \n",
    "    return im_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">\n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d\"> solvePNP </a>\n",
    "    is the function that finds an object pose from 3D-2D point correspondences. <br>\n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d0c/group__calib3d.html#ga1019495a2c8d1743ed5cc23fa0daff8c\"> projectPoints </a>\n",
    "    is the function that The function computes the 2D projections of 3D points to the image plane. <br>\n",
    "</h2>\n",
    "\n",
    "$$ x = PX\\ where\\ P\\ is\\ the\\ camera\\ projection\\ matrix$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(2,cv2.CAP_DSHOW)\n",
    "cap = cv2.VideoCapture('./videos/monocular/chess_board.mp4')\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "vdo = cv2.VideoCapture('./videos/monocular/aruco.mp4')\n",
    "_, im_src = vdo.read()\n",
    "# im_src = cv2.imread('./images/Grumpy-cat.jpg')\n",
    "im_src_size = im_src.shape[:2]\n",
    "src_points = np.float32([[0,0], [im_src_size[1],0],[im_src_size[1], im_src_size[0]] ,[0, im_src_size[0]] ])\n",
    "while cap.isOpened() :\n",
    "    ret1, frame = cap.read()\n",
    "    ret2, im_src = vdo.read()\n",
    "    if ret1 and ret2 :\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        img = frame.copy()\n",
    "        found, corners = cv2.findChessboardCorners(gray, chessboard_size,None)\n",
    "        if found :\n",
    "            corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),term_criteria)\n",
    "            \n",
    "            dst_points = np.float32([corners2[0], corners2[7], corners2[47], corners2[40]])\n",
    "            # Find the rotation and translation vectors.\n",
    "            ret,rvecs, tvecs = cv2.solvePnP(objp, corners2, K, dist)\n",
    "            # project 3D points to image plane\n",
    "            axis_imgpts, _ = cv2.projectPoints(axis, rvecs, tvecs, K, dist)\n",
    "            box_imgpts, _ = cv2.projectPoints(box, rvecs, tvecs, K, dist)\n",
    "            \n",
    "            augmented = augmented_image(img, im_src, src_points, dst_points)\n",
    "            img = cv2.drawChessboardCorners(frame, chessboard_size, corners2, ret)\n",
    "            # img = draw_axis(frame,corners2,axis_imgpts)\n",
    "            # img = draw_box(img, corners, box_imgpts)\n",
    "            cv2.imshow('augmented', augmented)\n",
    "            \n",
    "    \n",
    "        cv2.imshow('Chessboard Pose estimation', img)\n",
    "        if cv2.waitKey(33) & 0xff == 27 :\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center\">\n",
    "    <a href=\"http://www.uco.es/investiga/grupos/ava/node/26\"> ArUco</a> is that stand for \"Augmented Reality University of Cordoba\" <br>\n",
    "    is an opensource project for augmented reality and image-based object pose estimation <br>\n",
    "    For this project, it has been built into the open CV as well. It can be referenced from the following \n",
    "    <a href = \"https://docs.opencv.org/4.5.3/d9/d6a/group__aruco.html\"> ArUco Marker Detection </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KMUTNB\\IEE\\Image Processing\\010723305-Computer-Vision/camera_params/new-camera_params/\n",
      "Camera matrix\n",
      "[[1.29968396e+03 0.00000000e+00 6.32814378e+02]\n",
      " [0.00000000e+00 1.29933998e+03 4.74721903e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Len distortion\n",
      "[[-0.40795692  0.24741507  0.00200567 -0.00202735 -0.09445672]]\n"
     ]
    }
   ],
   "source": [
    "params_dir = os.getcwd()+'/camera_params/new-camera_params/'\n",
    "print(params_dir)\n",
    "\n",
    "#load camera parameters\n",
    "K = np.load(params_dir+'K.npy')\n",
    "dist = np.load(params_dir+'dist.npy')\n",
    "\n",
    "print(\"Camera matrix\")\n",
    "print(K)\n",
    "print(\"Len distortion\")\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "AruCo_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_4X4_50)\n",
    "AruCo_params = cv2.aruco.DetectorParameters_create()\n",
    "board = cv2.aruco.GridBoard_create(3, 4, 0.05, 0.0075, AruCo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(img, pose, dy, text) :\n",
    "    x0 = pose[0]\n",
    "    y0 = pose[1]\n",
    "    for i, line in enumerate(text.split('\\n')) :\n",
    "        y = y0 + i*dy\n",
    "        cv2.putText(img, line, np.int32([x0, y]), cv2.FONT_HERSHEY_COMPLEX, 0.75, (50,200,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(2,cv2.CAP_DSHOW)\n",
    "cap = cv2.VideoCapture('./videos/monocular/aruco.mp4')\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while cap.isOpened() :\n",
    "    ret, frame = cap.read()\n",
    "    if ret :\n",
    "        img = frame.copy()\n",
    "        \n",
    "        markerCorners, markerIds, rejectedCandidates = cv2.aruco.detectMarkers(frame, AruCo_dict, parameters = AruCo_params)\n",
    "        \n",
    "        if len(markerCorners) > 0:\n",
    "            img = cv2.aruco.drawDetectedMarkers(frame, markerCorners)\n",
    "            rvecs, tvecs, points = cv2.aruco.estimatePoseSingleMarkers(markerCorners , 0.05, K, dist)\n",
    "            for (rvec, tvec, id, corner) in zip(rvecs, tvecs, markerIds, markerCorners) :\n",
    "                img = cv2.aruco.drawAxis(frame, K, dist, rvec, tvec, 0.05)\n",
    "                x = tvec[0,0]\n",
    "                y = tvec[0,1]\n",
    "                z = tvec[0,2]\n",
    "                text = \"id: {}\\n pose:\\n {:.3f}\\n {:.3f}\\n {:.3f}\".format(id, x, y, z)\n",
    "                cX = (corner[0,0][0] + corner[0,2][0]) / 2\n",
    "                cY = (corner[0,0][1] + corner[0,2][1]) / 2\n",
    "                write_text(img, (cX, cY), 20, text)\n",
    "            ret, brvec, btvec = cv2.aruco.estimatePoseBoard(markerCorners, markerIds, board, K, dist, rvecs, tvecs) \n",
    "            if ret :\n",
    "                img = cv2.aruco.drawAxis(frame, K, dist, brvec, btvec, 0.05)\n",
    "    \n",
    "        cv2.imshow('Frame', img)\n",
    "        if cv2.waitKey(1) & 0xff == 27 :\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "    \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bde8de4267fa2326a064a32ec8bfaf139654fccd7a5afd7688ec5ca48592f22e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
